{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_limited_vocab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNS7f+jUQicBYCwETh0ej+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/ProjektarbeitML/blob/main/LSTM_limited_vocab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7k0zcJoyD3R"
      },
      "source": [
        "Quelle fÃ¼r Twitter-Daten: https://github.com/uds-lsv/GermEval-2018-Data/\n",
        "\n",
        "Michael Wiegand, Melanie Siegel, and Josef Ruppenhofer: \"Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language\", in Proceedings of the GermEval, 2018, Vienna, Austria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "#import matplotlib.pyplot as plt -> fÃ¼r evtl Visualisierungen\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "#!pip install Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#!pip install pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWJQ1iga_vS"
      },
      "source": [
        "Einige Parameter fÃ¼r das Netz setzen.. https://becominghuman.ai/creating-your-own-neural-network-using-tensorflow-fa8ca7cc4d0e "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "training_epochs = 10\n",
        "batch_size = 32\n",
        "max_length = 60\n",
        "max_vocab_size=6000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e71860-deb3-447d-e489-ac2c884d745a"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\n",
            "9068544/Unknown - 1s 0us/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d87ae6e-fe17-4e72-ed5e-ac57136e3763"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LICENSE.md',\n",
              " 'results.pdf',\n",
              " 'germeval2018.test.txt',\n",
              " 'README.md',\n",
              " 'submissions.tgz',\n",
              " 'germeval2018.training.txt',\n",
              " 'survey.pdf',\n",
              " 'guidelines-iggsa-shared.pdf',\n",
              " 'evaluationScriptGermeval2018.pl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ðŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ«\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ðŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’ƒðŸ½\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ðŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ðŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘©â€â¤ï¸â€ðŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ðŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸŽ‰\", \" \",string) \n",
        "   string = re.sub(\"ðŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‡©ðŸ‡ª\", \" \",string)  \n",
        "   \n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "   #sentimentsForTraining.append(findSentiment.group(0))\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "  #sentimentsForTesting.append(sent.group(0))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xUEbZj2XTvZ"
      },
      "source": [
        "Ansehen (wegen num_words) https://github.com/keras-team/keras/issues/8092 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3hFi7waTv5m"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=6000, oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "validation_size = 500\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_training = pad_sequences(training_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "#print(len(padded_training))\n",
        "\n",
        "validation_sequences = padded_training[0:validation_size]\n",
        "validation_labels = training_labels[0:validation_size]\n",
        "\n",
        "padded_training = padded_training[validation_size:]\n",
        "training_labels = training_labels[validation_size:]\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "padded_testing = pad_sequences(testing_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "#print(validation_sequences[499])\n",
        "#print(padded_training[0])\n",
        "#print(len(validation_labels))\n",
        "#print(len(training_labels))\n",
        "\n",
        "\n",
        "nppadded_training = np.array(padded_training)\n",
        "nptraining_labels = np.array(training_labels)\n",
        "\n",
        "nppadded_validation = np.array(validation_sequences)\n",
        "npvalidation_labels = np.array(validation_labels)\n",
        "\n",
        "nppadded_testing = np.array(padded_testing)\n",
        "nptesting_labels = np.array(testing_labels)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb1Le2oiUsVA",
        "outputId": "d42fea59-fc59-45bf-e1eb-de614c8b50f2"
      },
      "source": [
        "#print(statementsForTraining[2])\n",
        "\n",
        "#print(nppadded_training[4])\n",
        "#print(nppadded_training.shape)\n",
        "\n",
        "print(nptraining_labels[4])\n",
        "#print(nppadded_testing.shape)\n",
        "\n",
        "#print(word_index) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj8KEkavjm-D",
        "outputId": "4bc31965-ecdb-4e2a-fdb0-ac87b6e5c3a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.listdir(\"/content/drive/MyDrive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['germeval_training.txt',\n",
              " 'glove.twitter.27B.50d.txt',\n",
              " 'glove.twitter.27B.200d.txt',\n",
              " 'glove.840B.300d.txt',\n",
              " 'tensorboard.gdoc',\n",
              " 'keras.gdoc',\n",
              " 'CNN embedded.ipynb',\n",
              " 'CNN embedded_limited vocab.ipynb',\n",
              " 'LSTM.ipynb',\n",
              " 'LSTM_limited_vocab.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB_6sr3_jhPK",
        "outputId": "994b2eff-faa4-4ac4-c3f1-a00ed93a1881"
      },
      "source": [
        "#GrÃ¶ÃŸe Vokabel -> wordindex + 2 (weil padding + OOV) \n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "#vocabulary_size = len(word_index)+2\n",
        "vocabulary_size = 6000\n",
        "\n",
        "# dann erstell ich ein WÃ¶rterbuch mit Namen \"embedding_vector\", dort sind dann\n",
        "#die keys drinnen, die in glove-Datei drinnen sind mit dem entsprechenden Key\n",
        "\n",
        "embedding_index_glove = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "  value = line.split()\n",
        "  word = value[0]\n",
        "  coef = np.asarray(value[1:],dtype='float32')\n",
        "  embedding_index_glove[word] = coef\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove))\n",
        "\n",
        "#Dann noch eine Embedding-Matrix erstellen\n",
        "#zweiter Wert = Embedding-Dimension der Datei, in dem Fall 200\n",
        "\n",
        "glove_matrix = np.zeros((vocabulary_size,200))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size-1:\n",
        "      break\n",
        "    else:\n",
        "      embedding_value = embedding_index_glove.get(word)\n",
        "      if embedding_value is not None:\n",
        "        glove_matrix[index] = embedding_value\n",
        "        hits+=1\n",
        "      else:\n",
        "        misses+=1\n",
        "\n",
        "print(\"hits %d and %d misses\"%(hits,misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1193514 gefunden: \n",
            "hits 4103 and 1896 misses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekXQeS2rqN01"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6LLQhR-tQ_3"
      },
      "source": [
        "output-dimension -> Werte variieren.. Zuerst 16, da war die Accuracy nur etwas Ã¼ber 60, bei LSTM 10.\n",
        "Dann Output-dim auf 120 und LSTM auf 10 -> Accuracy fast bei 90%\n",
        "\n",
        "dann LSTM auf 265 und dim auf 120 -> knapp Ã¼ber 60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    data_true = K.ones_like(data_true)\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    data_true = K.ones_like(data_true)\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFwXT1wypY_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f929e09-97a2-4eb7-ea68-186a3081043d"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "print(len(word_index))\n",
        "model_LSTM = tf.keras.Sequential()\n",
        "#model_LSTM.add(tf.keras.layers.Embedding(input_dim=15132, output_dim=100, input_length=max_length))\n",
        "model_LSTM.add(tf.keras.layers.Embedding(vocabulary_size, output_dim=200, input_length=60, embeddings_initializer = keras.initializers.Constant(glove_matrix), trainable= False))\n",
        "model_LSTM.add(tf.keras.layers.LSTM(95, activation=\"relu\", use_bias=True, return_sequences=True))\n",
        "model_LSTM.add(tf.keras.layers.LSTM(95))\n",
        "model_LSTM.add(tf.keras.layers.Dropout(0.5))\n",
        "model_LSTM.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_LSTM.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',metrics_recall,metrics_precision,metrics_f1])\n",
        "print(model_LSTM.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15130\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 60, 200)           1200000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 60, 95)            112480    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 95)                72580     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 95)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                6144      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,391,269\n",
            "Trainable params: 191,269\n",
            "Non-trainable params: 1,200,000\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-OF4Y5ie4R"
      },
      "source": [
        "model_LSTM.layers[1].get_weights()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyEkmklD2S-n"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUICNHA62OJM"
      },
      "source": [
        "logs_base_dir = \"./logs\"\n",
        "callbackForTB = tf.keras.callbacks.TensorBoard(logs_base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weZe8g7urGGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e20218a-353e-41e8-91d6-6566ad1015cd"
      },
      "source": [
        "model_LSTM.fit(nppadded_training, nptraining_labels, batch_size=128, epochs=10, validation_data=(nppadded_testing, nptesting_labels), callbacks=[callbackForTB])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36/36 [==============================] - 17s 376ms/step - loss: 1.5469 - accuracy: 0.6476 - metrics_recall: 0.0584 - metrics_precision: 0.4933 - metrics_f1: 0.1020 - val_loss: 0.6455 - val_accuracy: 0.6597 - val_metrics_recall: 0.0000e+00 - val_metrics_precision: 0.0000e+00 - val_metrics_f1: 0.0000e+00\n",
            "Epoch 2/10\n",
            "36/36 [==============================] - 12s 347ms/step - loss: 0.6552 - accuracy: 0.6379 - metrics_recall: 0.0809 - metrics_precision: 0.9969 - metrics_f1: 0.1455 - val_loss: 0.6645 - val_accuracy: 0.6597 - val_metrics_recall: 0.0000e+00 - val_metrics_precision: 0.0000e+00 - val_metrics_f1: 0.0000e+00\n",
            "Epoch 3/10\n",
            "36/36 [==============================] - 12s 345ms/step - loss: 0.6575 - accuracy: 0.6460 - metrics_recall: 0.0271 - metrics_precision: 0.8557 - metrics_f1: 0.0507 - val_loss: 0.6519 - val_accuracy: 0.6597 - val_metrics_recall: 0.0000e+00 - val_metrics_precision: 0.0000e+00 - val_metrics_f1: 0.0000e+00\n",
            "Epoch 4/10\n",
            "36/36 [==============================] - 12s 346ms/step - loss: 0.6442 - accuracy: 0.6620 - metrics_recall: 0.0019 - metrics_precision: 0.1126 - metrics_f1: 0.0037 - val_loss: 0.6404 - val_accuracy: 0.6597 - val_metrics_recall: 0.0000e+00 - val_metrics_precision: 0.0000e+00 - val_metrics_f1: 0.0000e+00\n",
            "Epoch 5/10\n",
            "36/36 [==============================] - 12s 346ms/step - loss: 0.6334 - accuracy: 0.6731 - metrics_recall: 0.0049 - metrics_precision: 0.3191 - metrics_f1: 0.0094 - val_loss: 0.6376 - val_accuracy: 0.6628 - val_metrics_recall: 0.0035 - val_metrics_precision: 0.2143 - val_metrics_f1: 0.0068\n",
            "Epoch 6/10\n",
            "36/36 [==============================] - 12s 346ms/step - loss: 0.6250 - accuracy: 0.6786 - metrics_recall: 0.0447 - metrics_precision: 0.9128 - metrics_f1: 0.0772 - val_loss: 0.7222 - val_accuracy: 0.6509 - val_metrics_recall: 0.0846 - val_metrics_precision: 1.0000 - val_metrics_f1: 0.1553\n",
            "Epoch 7/10\n",
            "36/36 [==============================] - 12s 344ms/step - loss: 0.6805 - accuracy: 0.6727 - metrics_recall: 0.0694 - metrics_precision: 0.6562 - metrics_f1: 0.1172 - val_loss: 0.6412 - val_accuracy: 0.6608 - val_metrics_recall: 0.0013 - val_metrics_precision: 0.1429 - val_metrics_f1: 0.0026\n",
            "Epoch 8/10\n",
            "36/36 [==============================] - 12s 348ms/step - loss: 0.6296 - accuracy: 0.6683 - metrics_recall: 0.0350 - metrics_precision: 0.9363 - metrics_f1: 0.0657 - val_loss: 0.6656 - val_accuracy: 0.6642 - val_metrics_recall: 0.0048 - val_metrics_precision: 0.3929 - val_metrics_f1: 0.0095\n",
            "Epoch 9/10\n",
            "36/36 [==============================] - 12s 347ms/step - loss: 0.6098 - accuracy: 0.6844 - metrics_recall: 0.0509 - metrics_precision: 0.9427 - metrics_f1: 0.0894 - val_loss: 0.6206 - val_accuracy: 0.6730 - val_metrics_recall: 0.0170 - val_metrics_precision: 0.9643 - val_metrics_f1: 0.0333\n",
            "Epoch 10/10\n",
            "36/36 [==============================] - 12s 347ms/step - loss: 0.5822 - accuracy: 0.7145 - metrics_recall: 0.1223 - metrics_precision: 1.0000 - metrics_f1: 0.2057 - val_loss: 0.5901 - val_accuracy: 0.6897 - val_metrics_recall: 0.0700 - val_metrics_precision: 1.0000 - val_metrics_f1: 0.1300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdbd4dce610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOgPSige2PSk"
      },
      "source": [
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gATYCGKxZUrH"
      },
      "source": [
        "from tensorboard import notebook\n",
        "notebook.list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRzyrHJ5ZYPZ"
      },
      "source": [
        "notebook.display(port=6006, height=1000) \n",
        "#error.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTGNmRTIYEhq"
      },
      "source": [
        "#!pip install tensorboardcolab\n",
        "#from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "#tbc=TensorBoardColab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "results = model_LSTM.evaluate(nppadded_testing, nptesting_labels, batch_size=batch_size)\n",
        "print(\"test loss, test acc:\",results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "model_LSTM.predict(nppadded_testing[10:16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTlZpSlfZRGt"
      },
      "source": [
        "print(testing_labels[10:16])\n",
        "#print(statementsForTesting[4:7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kSOvCeaszMR"
      },
      "source": [
        ""
      ]
    }
  ]
}