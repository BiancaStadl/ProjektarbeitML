{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding Glove Vergleich.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJsfR3BI0lh9z49yFo2OSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/ProjektarbeitML/blob/main/Embedding_Glove_Vergleich.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-vGgrBIlVL7"
      },
      "source": [
        "GloVe embeddings taken from:\n",
        "\n",
        "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
        "\n",
        "hatespeechdata taken from:\n",
        " Wiegand, M., Siegel, M. and Ruppenhofer, J., 2018. Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language. In: Proceedings of GermEval 2018, 14th Conference on Natural Language Processing (KONVENS 2018). Vienna, Austria: Research Gate. available on: https://github.com/uds-lsv/GermEval-2018-Data (last checked: 09.05.2021)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3IL5SYs86a0"
      },
      "source": [
        "#import matplotlib.pyplot as plt -> fÃ¼r evtl Visualisierungen\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import losses \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "#!pip install Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#!pip install pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a9hbeXc-IS9"
      },
      "source": [
        "\n",
        "max_length = 60"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAqt_sYG-MYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f596c549-ae86-4ba0-c4e6-0a501434a504"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\n",
            "6635520/Unknown - 1s 0us/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfpbvxr0-Ny-",
        "outputId": "feb1ef14-04b1-4548-a8db-28fc8f55c57e"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['results.pdf',\n",
              " 'evaluationScriptGermeval2018.pl',\n",
              " 'README.md',\n",
              " 'germeval2018.test.txt',\n",
              " 'LICENSE.md',\n",
              " 'survey.pdf',\n",
              " 'submissions.tgz',\n",
              " 'germeval2018.training.txt',\n",
              " 'guidelines-iggsa-shared.pdf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsprUe9R-QC2"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yynliNKf-TKb"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ðŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ«\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ðŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’ƒðŸ½\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ðŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ðŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘©â€â¤ï¸â€ðŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ðŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸŽ‰\", \" \",string) \n",
        "   string = re.sub(\"ðŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‡©ðŸ‡ª\", \" \",string)  \n",
        "   \n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y017OXHR-WOM"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "   #sentimentsForTraining.append(findSentiment.group(0))\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[9])\n",
        "#print(training_labels[9])\n",
        "\n",
        "#print(len(training_sentences))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V967mYOb-ZoM",
        "outputId": "cae24019-756d-4e8b-e3e0-95baa4efa890"
      },
      "source": [
        "##do the same with testdata\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "    \n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "  #sentimentsForTesting.append(sent.group(0))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(testing_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebQeBcUU-bex",
        "outputId": "e59a7f23-069b-4acd-9d91-2fd971248c25"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "#creating a word index - nur die Trainigsdaten\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "validation_size = 500\n",
        "\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_training = pad_sequences(training_sequences, maxlen=max_length, padding='post')\n",
        "print(len(padded_training))\n",
        "\n",
        "validation_sequences = padded_training[0:validation_size]\n",
        "validation_labels = training_labels[0:validation_size]\n",
        "\n",
        "padded_training = padded_training[validation_size:]\n",
        "training_labels = training_labels[validation_size:]\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "padded_testing = pad_sequences(testing_sequences, maxlen=max_length,  padding='post')\n",
        "\n",
        "#print(validation_sequences[499])\n",
        "print(padded_training[0])\n",
        "#print(len(validation_labels))\n",
        "#print(len(training_labels))\n",
        "\n",
        "\n",
        "nppadded_training = np.array(padded_training)\n",
        "nptraining_labels = np.array(training_labels)\n",
        "\n",
        "nppadded_validation = np.array(validation_sequences)\n",
        "npvalidation_labels = np.array(validation_labels)\n",
        "\n",
        "nppadded_testing = np.array(padded_testing)\n",
        "nptesting_labels = np.array(testing_labels)\n",
        "\n",
        "\n",
        "print(len(nppadded_training))\n",
        "print(len(nptraining_labels))\n",
        "print(len(word_index))\n",
        "\n",
        "#print(statementsForTraining[2])\n",
        "#print(nppadded_training[4])\n",
        "#print(nppadded_training.shape)\n",
        "#print(nptraining_labels[4])\n",
        "#print(nppadded_testing.shape)\n",
        "#print(word_index) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5009\n",
            "[  12 3993   11   41 6730 1042    4    5  202   39    3 2930   49 1363\n",
            "  812  495 3994    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "4509\n",
            "4509\n",
            "15130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiXKgZbL-d7-",
        "outputId": "3b45ea64-f616-4b40-e96a-9a5b4ead009a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.listdir(\"/content/drive/MyDrive/Colab Notebooks\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['germeval_training.txt',\n",
              " 'glove.twitter.27B.50d.txt',\n",
              " 'glove.twitter.27B.200d.txt',\n",
              " 'glove.6B.200d.txt',\n",
              " 'glove.840B.300d.txt',\n",
              " 'tensorboard.gdoc',\n",
              " 'keras.gdoc',\n",
              " 'Vectorization CNN embedded_limited_vocab.ipynb',\n",
              " 'LSTM_limited_vocab.ipynb',\n",
              " 'LSTM.ipynb',\n",
              " 'CNN embedded_limited_vocab.ipynb',\n",
              " 'CNN embedded.ipynb',\n",
              " 'glove.42B.300d.txt',\n",
              " 'Embedding Glove Vergleich.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QerPH3rM-epe"
      },
      "source": [
        "First Embedding with Glove Twitter 200d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKufhYZfGBtM"
      },
      "source": [
        "vocabulary_size = len(word_index)+2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t-HC4UI-igA",
        "outputId": "03554a04-1d21-46f9-bec8-fdfdaf161a7b"
      },
      "source": [
        "#GrÃ¶ÃŸe Vokabel -> wordindex + 2 (weil padding + OOV) \n",
        "hits200 = 0\n",
        "misses200 = 0\n",
        "\n",
        "\n",
        "\n",
        "# dann erstell ich ein WÃ¶rterbuch mit Namen \"embedding_vector\", dort sind dann\n",
        "#die keys drinnen, die in glove-Datei drinnen sind mit dem entsprechenden Key\n",
        "\n",
        "embedding_index_glove = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "  value = line.split()\n",
        "  word = value[0]\n",
        "  coef = np.asarray(value[1:],dtype='float32')\n",
        "  embedding_index_glove[word] = coef\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove))\n",
        "\n",
        "#Dann noch eine Embedding-Matrix erstellen\n",
        "#zweiter Wert = Embedding-Dimension der Datei, in dem Fall 200\n",
        "\n",
        "glove_matrix = np.zeros((vocabulary_size,200))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_value = embedding_index_glove.get(word)\n",
        "    if embedding_value is not None:\n",
        "      glove_matrix[index] = embedding_value\n",
        "      hits200+=1\n",
        "    else:\n",
        "      misses200+=1\n",
        "\n",
        "print(\"hits %d and %d misses\"%(hits200,misses200))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1193514 gefunden: \n",
            "hits 6747 and 8383 misses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQw3PgUG-oK5"
      },
      "source": [
        "Second embedding try with glove.840B.300d.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5HLGqsQ-wGG",
        "outputId": "e808cbe1-99bc-474e-fab4-4d474005a2ad"
      },
      "source": [
        "hits300 = 0\n",
        "misses300 = 0\n",
        "\n",
        "embedding_index_glove300 = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.42B.300d.txt')\n",
        "for line300 in f:\n",
        "  value300 = line300.split()\n",
        "  word300 = value300[0]\n",
        "  coef300 = np.asarray(value300[1:],dtype='float32')\n",
        "  embedding_index_glove300[word300] = coef300\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove300))\n",
        "\n",
        "\n",
        "glove_matrix300 = np.zeros((vocabulary_size,300))\n",
        "for word300, index300 in tokenizer.word_index.items():\n",
        "    embedding_value300 = embedding_index_glove300.get(word300)\n",
        "    if embedding_value300 is not None:\n",
        "      glove_matrix300[index300] = embedding_value300\n",
        "      hits300+=1\n",
        "    else:\n",
        "      misses300+=1\n",
        "\n",
        "print(\"hits %d and %d misses for glove 300\"%(hits300,misses300))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1917494 gefunden: \n",
            "hits 6963 and 8167 misses for glove 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUyK1xTV-5nP"
      },
      "source": [
        "weiterer Test mit 50 Dim, sollte gleich sein wie zuvor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gkF6TkE-8gY",
        "outputId": "a6d6afbf-5ec2-499f-ec9e-e8f74f97fcad"
      },
      "source": [
        "hits50 = 0\n",
        "misses50 = 0\n",
        "\n",
        "embedding_index_glove50 = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.50d.txt')\n",
        "for line50 in f:\n",
        "  value50 = line50.split()\n",
        "  word50 = value50[0]\n",
        "  coef50 = np.asarray(value50[1:],dtype='float32')\n",
        "  embedding_index_glove50[word50] = coef50\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove50))\n",
        "\n",
        "\n",
        "glove_matrix50 = np.zeros((vocabulary_size,50))\n",
        "for word50, index50 in tokenizer.word_index.items():\n",
        "    embedding_value50 = embedding_index_glove50.get(word50)\n",
        "    if embedding_value50 is not None:\n",
        "      glove_matrix50[index50] = embedding_value50\n",
        "      hits50+=1\n",
        "    else:\n",
        "      misses50+=1\n",
        "\n",
        "print(\"hits %d and %d misses for glove 50 \"%(hits50,misses50))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1193514 gefunden: \n",
            "hits 6747 and 8383 misses for glove 50 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw3eVM2cCC2A"
      },
      "source": [
        "und noch mit glove 6B (200d, gibt auch andere Dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpvvkuUhCGxZ",
        "outputId": "3d44cfa8-26d4-4cfe-fe5f-226b07f98d48"
      },
      "source": [
        "\n",
        "hits6b = 0\n",
        "misses6b = 0\n",
        "\n",
        "vocabulary_size = len(word_index)+2\n",
        "\n",
        "embedding_index_glove6b = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.6B.200d.txt')\n",
        "for line6b in f:\n",
        "  value6b = line6b.split()\n",
        "  word6b = value6b[0]\n",
        "  coef6b = np.asarray(value6b[1:],dtype='float32')\n",
        "  embedding_index_glove6b[word6b] = coef6b\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove6b))\n",
        "\n",
        "\n",
        "glove_matrix6b = np.zeros((vocabulary_size,200))\n",
        "for word6b, index6b in tokenizer.word_index.items():\n",
        "    embedding_value6b = embedding_index_glove6b.get(word6b)\n",
        "    if embedding_value6b is not None:\n",
        "      glove_matrix6b[index6b] = embedding_value6b\n",
        "      hits6b+=1\n",
        "    else:\n",
        "      misses6b+=1\n",
        "\n",
        "print(\"hits %d and %d misses for glove 6b\"%(hits6b,misses6b))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000 gefunden: \n",
            "hits 3721 and 11409 misses for glove 6b\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}