{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoLCeIGWwjiulBPcUVMPOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/ProjektarbeitML/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7k0zcJoyD3R"
      },
      "source": [
        "Quelle fÃ¼r Twitter-Daten: https://github.com/uds-lsv/GermEval-2018-Data/\n",
        "\n",
        "Michael Wiegand, Melanie Siegel, and Josef Ruppenhofer: \"Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language\", in Proceedings of the GermEval, 2018, Vienna, Austria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "#import matplotlib.pyplot as plt -> fÃ¼r evtl Visualisierungen\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "#!pip install Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#!pip install pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWJQ1iga_vS"
      },
      "source": [
        "Einige Parameter fÃ¼r das Netz setzen.. https://becominghuman.ai/creating-your-own-neural-network-using-tensorflow-fa8ca7cc4d0e "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "training_epochs = 10\n",
        "batch_size = 32\n",
        "max_length = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad3f316-9d63-40aa-f9c4-ae2012b7d001"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['evaluationScriptGermeval2018.pl',\n",
              " 'LICENSE.md',\n",
              " 'guidelines-iggsa-shared.pdf',\n",
              " 'germeval2018.training.txt',\n",
              " 'results.pdf',\n",
              " 'survey.pdf',\n",
              " 'submissions.tgz',\n",
              " 'README.md',\n",
              " 'germeval2018.test.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ðŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ«\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ðŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’ƒðŸ½\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ðŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ðŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘©â€â¤ï¸â€ðŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ðŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸŽ‰\", \" \",string) \n",
        "   string = re.sub(\"ðŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‡©ðŸ‡ª\", \" \",string)  \n",
        "   \n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "   #sentimentsForTraining.append(findSentiment.group(0))\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "  #sentimentsForTesting.append(sent.group(0))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3hFi7waTv5m"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "validation_size = 500\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_training = pad_sequences(training_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "#print(len(padded_training))\n",
        "\n",
        "validation_sequences = padded_training[0:validation_size]\n",
        "validation_labels = training_labels[0:validation_size]\n",
        "\n",
        "padded_training = padded_training[validation_size:]\n",
        "training_labels = training_labels[validation_size:]\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "padded_testing = pad_sequences(testing_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "#print(validation_sequences[499])\n",
        "#print(padded_training[0])\n",
        "#print(len(validation_labels))\n",
        "#print(len(training_labels))\n",
        "\n",
        "\n",
        "nppadded_training = np.array(padded_training)\n",
        "nptraining_labels = np.array(training_labels)\n",
        "\n",
        "nppadded_validation = np.array(validation_sequences)\n",
        "npvalidation_labels = np.array(validation_labels)\n",
        "\n",
        "nppadded_testing = np.array(padded_testing)\n",
        "nptesting_labels = np.array(testing_labels)\n",
        "\n",
        "#print(statementsForTraining[2])\n",
        "\n",
        "#print(nppadded_training[4])\n",
        "#print(nppadded_training.shape)\n",
        "\n",
        "#print(nptraining_labels[4])\n",
        "#print(nppadded_testing.shape)\n",
        "\n",
        "#print(word_index) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj8KEkavjm-D",
        "outputId": "22a24dd9-6c46-4da3-b465-843b3685b24d"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")\n",
        "os.listdir(\"/content/drive/MyDrive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test 2.ipynb',\n",
              " 'germeval_training.txt',\n",
              " 'TestNote.ipynb',\n",
              " 'glove.twitter.27B.50d.txt',\n",
              " 'glove.twitter.27B.200d.txt',\n",
              " 'glove.840B.300d.txt',\n",
              " 'CNN_try.ipynb',\n",
              " 'tensorboard.gdoc',\n",
              " 'keras.gdoc',\n",
              " 'CNN embedded.ipynb',\n",
              " 'LSTM.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB_6sr3_jhPK",
        "outputId": "1b254250-00f1-44a1-caf9-2510f2036a96"
      },
      "source": [
        "#GrÃ¶ÃŸe Vokabel -> wordindex + 2 (weil padding + OOV) \n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "vocabulary_size = len(word_index)+2\n",
        "\n",
        "# dann erstell ich ein WÃ¶rterbuch mit Namen \"embedding_vector\", dort sind dann\n",
        "#die keys drinnen, die in glove-Datei drinnen sind mit dem entsprechenden Key\n",
        "\n",
        "embedding_index_glove = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "  value = line.split(' ')\n",
        "  word = value[0]\n",
        "  coef = np.array(value[1:],dtype='float32')\n",
        "  embedding_index_glove[word] = coef\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove))\n",
        "\n",
        "#Dann noch eine Embedding-Matrix erstellen\n",
        "#zweiter Wert = Embedding-Dimension der Datei, in dem Fall 200\n",
        "\n",
        "glove_matrix = np.zeros((vocabulary_size,200))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_value = embedding_index_glove.get(word)\n",
        "    if embedding_value is not None:\n",
        "      glove_matrix[index] = embedding_value\n",
        "      hits+=1\n",
        "    else:\n",
        "      misses+=1\n",
        "\n",
        "print(\"hits %d and %d misses\"%(hits,misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1193514 gefunden: \n",
            "hits 6747 and 8383 misses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekXQeS2rqN01"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6LLQhR-tQ_3"
      },
      "source": [
        "output-dimension -> Werte variieren.. Zuerst 16, da war die Accuracy nur etwas Ã¼ber 60, bei LSTM 10.\n",
        "Dann Output-dim auf 120 und LSTM auf 10 -> Accuracy fast bei 90%\n",
        "\n",
        "dann LSTM auf 265 und dim auf 120 -> knapp Ã¼ber 60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFwXT1wypY_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f36bb50-9119-4c42-a23e-f72c461cef4c"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "print(len(word_index))\n",
        "model_LSTM = tf.keras.Sequential()\n",
        "#model_LSTM.add(tf.keras.layers.Embedding(input_dim=15132, output_dim=100, input_length=max_length))\n",
        "model_LSTM.add(tf.keras.layers.Embedding(vocabulary_size, output_dim=200, input_length=60, embeddings_initializer = keras.initializers.Constant(glove_matrix), trainable= False))\n",
        "#model_LSTM.add(tf.keras.layers.Conv1D(filters=10, kernel_size=3,activation='relu'))\n",
        "#model_LSTM.add(tf.keras.layers.MaxPooling1D())\n",
        "#model_LSTM.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.3, recurrent_dropout= 0.3, activation=\"sigmoid\")))\n",
        "#model_LSTM.add(tf.keras.layers.LSTM(265, activation=\"tanh\",recurrent_activation=\"sigmoid\",recurrent_dropout=0,unroll=\"false\",use_bias=\"True\"))\n",
        "model_LSTM.add(tf.keras.layers.LSTM(265, dropout=0.5, activation=\"tanh\", recurrent_activation=\"sigmoid\"))\n",
        "#model_LSTM.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model_LSTM.add(tf.keras.layers.Dense(64,activation='relu'))\n",
        "model_LSTM.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_LSTM.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15130\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 60, 200)           3026400   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 265)               493960    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                17024     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,537,449\n",
            "Trainable params: 511,049\n",
            "Non-trainable params: 3,026,400\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-OF4Y5ie4R"
      },
      "source": [
        "model_LSTM.layers[1].get_weights()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyEkmklD2S-n"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUICNHA62OJM"
      },
      "source": [
        "logs_base_dir = \"./logs\"\n",
        "callbackForTB = tf.keras.callbacks.TensorBoard(logs_base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weZe8g7urGGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8323f66-5370-44ab-d8d1-49646b0d434a"
      },
      "source": [
        "model_LSTM.fit(nppadded_training, nptraining_labels, batch_size=40, epochs=6, validation_data=(nppadded_testing, nptesting_labels), callbacks=[callbackForTB])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "113/113 [==============================] - 67s 572ms/step - loss: 0.6391 - accuracy: 0.6685 - val_loss: 0.6485 - val_accuracy: 0.6500\n",
            "Epoch 2/6\n",
            "113/113 [==============================] - 64s 562ms/step - loss: 0.6459 - accuracy: 0.6606 - val_loss: 0.6513 - val_accuracy: 0.6500\n",
            "Epoch 3/6\n",
            "113/113 [==============================] - 63s 561ms/step - loss: 0.6466 - accuracy: 0.6562 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
            "Epoch 4/6\n",
            "113/113 [==============================] - 63s 560ms/step - loss: 0.6416 - accuracy: 0.6659 - val_loss: 0.6488 - val_accuracy: 0.6500\n",
            "Epoch 5/6\n",
            "113/113 [==============================] - 63s 554ms/step - loss: 0.6382 - accuracy: 0.6669 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
            "Epoch 6/6\n",
            "113/113 [==============================] - 63s 554ms/step - loss: 0.6371 - accuracy: 0.6686 - val_loss: 0.6519 - val_accuracy: 0.6500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f346461d090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOgPSige2PSk"
      },
      "source": [
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gATYCGKxZUrH"
      },
      "source": [
        "from tensorboard import notebook\n",
        "notebook.list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRzyrHJ5ZYPZ"
      },
      "source": [
        "notebook.display(port=6006, height=1000) \n",
        "#error.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTGNmRTIYEhq"
      },
      "source": [
        "#!pip install tensorboardcolab\n",
        "#from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "#tbc=TensorBoardColab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f273971d-210e-4a21-e457-b1d6ab6a09e2"
      },
      "source": [
        "results = model_LSTM.evaluate(nppadded_testing, nptesting_labels, batch_size=batch_size)\n",
        "print(\"test loss, test acc:\",results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111/111 [==============================] - 11s 98ms/step - loss: 0.6479 - accuracy: 0.6597\n",
            "test loss, test acc: [0.6478647589683533, 0.6596828699111938]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "model_LSTM.predict(nppadded_testing[10:16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTlZpSlfZRGt"
      },
      "source": [
        "#print(testing_labels[10:16])\n",
        "#print(statementsForTesting[4:7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kSOvCeaszMR"
      },
      "source": [
        ""
      ]
    }
  ]
}