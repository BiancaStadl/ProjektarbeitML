{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTXmUppjNqEjWNJtLb1nlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/ProjektarbeitML/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7k0zcJoyD3R"
      },
      "source": [
        "GloVe embeddings taken from:\n",
        "\n",
        "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
        "\n",
        "hatespeechdata taken from (https://hatespeechdata.com/): Wiegand, M., Siegel, M. and Ruppenhofer, J., 2018. Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language. In: Proceedings of GermEval 2018, 14th Conference on Natural Language Processing (KONVENS 2018). Vienna, Austria: Research Gate. available on: https://github.com/uds-lsv/GermEval-2018-Data (last checked: 09.05.2021)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "#import matplotlib.pyplot as plt -> fÃ¼r evtl Visualisierungen\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "#!pip install Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#!pip install pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWJQ1iga_vS"
      },
      "source": [
        "Einige Parameter fÃ¼r das Netz setzen.. https://becominghuman.ai/creating-your-own-neural-network-using-tensorflow-fa8ca7cc4d0e "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ðŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ«\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ðŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’ƒðŸ½\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ðŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ðŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘©â€â¤ï¸â€ðŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ðŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ðŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ðŸŽ‰\", \" \",string) \n",
        "   string = re.sub(\"ðŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ðŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‡©ðŸ‡ª\", \" \",string)  \n",
        "   string = re.sub(\"ðŸ’”\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™ˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ðŸŸ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ›¶\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜“\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜³\", \" \",string)\n",
        "   string = re.sub(\"ðŸš€\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘Ž\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜Ž\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ðŸ“ˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜…\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜†\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™ŽðŸ¿\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘ŽðŸ½\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤­\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¤\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜²\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤®\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™„\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸŽ…\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘‹\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’ª\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜„\", \" \",string)\n",
        "   string = re.sub(\"ðŸ§\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜ \", \" \",string)\n",
        "   string = re.sub(\"ðŸŽˆ\", \" \",string)\n",
        "   string = re.sub(\"ðŸš‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ðŸš‡\", \" \",string)\n",
        "   string = re.sub(\"ðŸšŠ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤·\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜¥\", \" \",string)\n",
        "   string = re.sub(\"ðŸ™ƒ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ”©\", \" \",string)\n",
        "   string = re.sub(\"ðŸ”§\", \" \",string)\n",
        "   string = re.sub(\"ðŸ”¨\", \" \",string)\n",
        "   string = re.sub(\"ðŸ› \", \" \",string)\n",
        "   string = re.sub(\"ðŸ’“\", \" \",string)\n",
        "   string = re.sub(\"ðŸ’¡\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¥ƒ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¥‚\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜·\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤\", \" \",string)\n",
        "   string = re.sub(\"ðŸŒŽ\", \" \",string)\n",
        "   string = re.sub(\"ðŸ‘‘\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤›\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜€\", \" \",string)\n",
        "   string = re.sub(\"ðŸ›¤\", \" \",string)\n",
        "   string = re.sub(\"ðŸŽ„\", \" \",string)\n",
        "   string = re.sub(\"ðŸ“´\", \" \",string)\n",
        "   string = re.sub(\"ðŸŒ­\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤•\", \" \",string)\n",
        "   string = re.sub(\"ðŸ˜­\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¾\", \" \",string)\n",
        "   string = re.sub(\"ðŸž\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¦\", \" \",string)\n",
        "   string = re.sub(\"ðŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ðŸ•¯ï¸\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "   #sentimentsForTraining.append(findSentiment.group(0))\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "  #sentimentsForTesting.append(sent.group(0))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3hFi7waTv5m"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "validation_size = 500\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded_training = pad_sequences(training_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "#print(len(padded_training))\n",
        "\n",
        "validation_sequences = padded_training[0:validation_size]\n",
        "validation_labels = training_labels[0:validation_size]\n",
        "\n",
        "padded_training = padded_training[validation_size:]\n",
        "training_labels = training_labels[validation_size:]\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "padded_testing = pad_sequences(testing_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "#print(validation_sequences[499])\n",
        "#print(padded_training[0])\n",
        "#print(len(validation_labels))\n",
        "#print(len(training_labels))\n",
        "\n",
        "\n",
        "nppadded_training = np.array(padded_training)\n",
        "nptraining_labels = np.array(training_labels)\n",
        "\n",
        "nppadded_validation = np.array(validation_sequences)\n",
        "npvalidation_labels = np.array(validation_labels)\n",
        "\n",
        "nppadded_testing = np.array(padded_testing)\n",
        "nptesting_labels = np.array(testing_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klOlU-WKU1sL"
      },
      "source": [
        "\n",
        "#print(statementsForTraining[2])\n",
        "\n",
        "print(validation_sequences[4])\n",
        "print(statementsForTraining[4])\n",
        "#print(nppadded_training.shape)\n",
        "\n",
        "#print(nptraining_labels[4])\n",
        "#print(nppadded_testing.shape)\n",
        "\n",
        "#print(word_index) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj8KEkavjm-D"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.listdir(\"/content/drive/MyDrive/Colab Notebooks\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB_6sr3_jhPK"
      },
      "source": [
        "#GrÃ¶ÃŸe Vokabel -> wordindex + 2 (weil padding + OOV) \n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "vocabulary_size = len(word_index)+2\n",
        "\n",
        "# dann erstell ich ein WÃ¶rterbuch mit Namen \"embedding_vector\", dort sind dann\n",
        "#die keys drinnen, die in glove-Datei drinnen sind mit dem entsprechenden Key\n",
        "\n",
        "embedding_index_glove = {}\n",
        "f = open('/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "  value = line.split()\n",
        "  word = value[0]\n",
        "  coef = np.asarray(value[1:],dtype='float32')\n",
        "  embedding_index_glove[word] = coef\n",
        "\n",
        "print(\"%d gefunden: \"% len(embedding_index_glove))\n",
        "\n",
        "#Dann noch eine Embedding-Matrix erstellen\n",
        "#zweiter Wert = Embedding-Dimension der Datei, in dem Fall 200\n",
        "\n",
        "glove_matrix = np.zeros((vocabulary_size,200))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_value = embedding_index_glove.get(word)\n",
        "    if embedding_value is not None:\n",
        "      glove_matrix[index] = embedding_value\n",
        "      hits+=1\n",
        "    else:\n",
        "      misses+=1\n",
        "\n",
        "print(\"hits %d and %d misses\"%(hits,misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekXQeS2rqN01"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFwXT1wypY_B"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "#print(len(word_index))\n",
        "LSTM16052111ES = tf.keras.Sequential()\n",
        "#LSTM16052111ES.add(tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=100, input_length=max_length))\n",
        "LSTM16052111ES.add(tf.keras.layers.Embedding(vocabulary_size, output_dim=200, input_length=60, embeddings_initializer = keras.initializers.Constant(glove_matrix), trainable= False))\n",
        "#LSTM16052111ES.add(tf.keras.layers.Conv1D(filters=10, kernel_size=3,activation='relu'))\n",
        "#LSTM16052111ES.add(tf.keras.layers.MaxPooling1D())\n",
        "LSTM16052111ES.add(tf.keras.layers.LSTM(60, activation=\"tanh\", recurrent_activation=\"sigmoid\", use_bias=True,return_sequences=True))\n",
        "#LSTM16052111ES.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "LSTM16052111ES.add(tf.keras.layers.LSTM(60))\n",
        "LSTM16052111ES.add(tf.keras.layers.Dropout(0.5))\n",
        "#LSTM16052111ES.add(tf.keras.layers.Dense(65,activation='relu'))\n",
        "#added 070521\n",
        "LSTM16052111ES.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IuFfu-1LeDP"
      },
      "source": [
        "LSTM16052111ES.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy',metrics_recall,metrics_precision,metrics_f1])\n",
        "print(LSTM16052111ES.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-OF4Y5ie4R"
      },
      "source": [
        "#LSTM16052111ES.layers[1].get_weights()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyEkmklD2S-n"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUICNHA62OJM"
      },
      "source": [
        "logs_base_dir = \"./logs\"\n",
        "callbackForTB = tf.keras.callbacks.TensorBoard(logs_base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v88JJEqj4Crv"
      },
      "source": [
        "training_epochs = 30\n",
        "batch_size = 40\n",
        "validation_split=0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7dTW-AKhMpF"
      },
      "source": [
        "Early Stopping as defined in keras tensorflow documentation https://www.tensorflow.org/guide/keras/train_and_evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdCQwgfShMVG"
      },
      "source": [
        "callbackEarlyStopping = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0.001,\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weZe8g7urGGu"
      },
      "source": [
        "LSTM16052111ES.fit(nppadded_training, nptraining_labels, batch_size=batch_size, epochs=training_epochs, validation_split=validation_split, callbacks=[callbackForTB,callbackEarlyStopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOgPSige2PSk"
      },
      "source": [
        "#%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gATYCGKxZUrH"
      },
      "source": [
        "#from tensorboard import notebook\n",
        "#notebook.list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRzyrHJ5ZYPZ"
      },
      "source": [
        "#notebook.display(port=6006, height=1000) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTGNmRTIYEhq"
      },
      "source": [
        "#!pip install tensorboardcolab\n",
        "#from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "#tbc=TensorBoardColab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "(loss,accuracy, metrics_recall, metrics_precision,\n",
        "metrics_f1) = LSTM16052111ES.evaluate(nppadded_testing, nptesting_labels, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "LSTM_predict13 = LSTM16052111ES.predict(x=nppadded_testing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict13:\n",
        " # print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "prediction_rounded13 = np.round(LSTM_predict13)\n",
        "\n",
        "#for p in prediction_rounded13:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTlZpSlfZRGt"
      },
      "source": [
        "#print(testing_labels[10:16])\n",
        "#print(statementsForTesting[4:7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=nptesting_labels, y_pred=prediction_rounded13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='LSTM Confusion 260')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}